# -*- coding: utf-8 -*-
"""/networks_exercise_instagram.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/notebooks/networks_exercise_instagram.ipynb

# Preamble
"""

# STandard stuff
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
import itertools # Python's amazing iteration & combination library

# For visualization
!pip install -U bokeh
!pip install -q holoviews

# Import the libraries and link to the bokeh backend
import holoviews as hv
from holoviews import opts
hv.extension('bokeh')
from bokeh.plotting import show

# Setting the default figure size a bit larger
defaults = dict(width=750, height=750, padding=0.1,
                xaxis=None, yaxis=None)
hv.opts.defaults(
    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))

# Network Stuff
import networkx as nx
import community # `python-louvain` is implemented here
from networkx.algorithms import bipartite # bipartite NW algos

# Blockmodel Stuff
!wget https://github.com/CALDISS-AAU/sdsphd19_coursematerials/raw/master/wednesday_network-blockmodeling/blockmodeling_material.zip # downloading module and data files to googe drive session
!unzip 'blockmodeling_material.zip' # unzipping

# import the necessary modules
import blockmodeling as bm
import matplotlib.pyplot as plt
import scipy as sc
import scipy.cluster.hierarchy as sch

# API&Scraping&instagramm
!pip3 install instaloader # Installing instaloader
import instaloader
L = instaloader.Instaloader()

import requests as rq # The requests library handles "requests" to APIs similar to a browser that requests a webpage given a URL
from nltk.tokenize import TweetTokenizer # A bit of a transition into NLP. The tweet tokenizer from the NLTK library will help us extract the hashtags from post-text
tknzr = TweetTokenizer()

"""# Task

So guys, now its time to put it all together. Take the two notebooks by Carl, and Daniel, and Carl, and Ddo the following:

1. Extract Instagram Tag infos of your choice
2. Generate a bipartite User-Tag network
3. Project it on either the user or the tag mode (your choice)
4. Apply blockmodeling on it 

* [Networks general: Daniel](https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/notebooks/CALDISS_PHD_Intro_networks.ipynb#&offline=true&sandboxMode=true)
* [Blockmodels: Carl](https://colab.research.google.com/github/CALDISS-AAU/sdsphd19_coursematerials/blob/master/wednesday_network-blockmodeling/Lab_Blockmodeling.ipynb#&offline=true&sandboxMode=true)

# Getting the data
"""

# Instagram base url preffix
tagurl_prefix = 'https://www.instagram.com/explore/tags/'

# suffix to append to tag request url to retrieve data in JSON format
tagurl_suffix = '/?__a=1'

# suffix to end cursor when requesting posts by tag
tagurl_endcursor = '&max_id='

# a generic media post preffix (concat with media shortcode to view)
posturl_prefix = 'https://www.instagram.com/p/'

#
# Find your own instagramm tag to explore!!!!!
#
tags = ['iranprotests']

# urls to initial tags using the above url-components
queries = [ tagurl_prefix + tag + tagurl_suffix for tag in tags ]

len(queries)

# Getting the data
edges = []
for q in queries:    
    for i in range(10): # how many iterations/deepth ?
      r = rq.get(q).json()
      end_cursor = r['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']
      edges.extend(r['graphql']['hashtag']['edge_hashtag_to_media']['edges'])
      print(i,q)
      q = q + tagurl_endcursor + end_cursor

edges

post_dicts = [] #empty list

for post in edges: #iterate all raw posts

  if post['node']['edge_media_to_caption']['edges'] == []: # hop to the next if no text in the post
    continue
    
  post_dict = {} # empty dictionary
  id_owner = post['node']['owner']['id'] # pick out user-id
  shortcode = post['node']['shortcode'] # pick out short post identifier
  text = post['node']['edge_media_to_caption']['edges'][0]['node']['text'] # pick out post text
  
  # Pick hashtags from text
  tokens = tknzr.tokenize(text)
  tags = [x.strip('#') for x in tokens if x.startswith('#')]

  # fill in dictionary with values
  post_dict['id_owner'] = id_owner
  post_dict['shortcode'] = shortcode
  post_dict['tags'] = tags
  post_dict['text'] = text

  post_dicts.append(post_dict) #append the dictionary to a list of post-dictionaries

# Create DF
posts_df = pd.DataFrame(post_dicts)

# Remove hashtags that are not a hashtag (emptyspace & mistakes)
posts_df['tags'] = posts_df['tags'].map(lambda t: [x for x in t if x.isalnum()])

# Kick out posts with 0 hashtags
posts_df = posts_df[posts_df['tags'].map(len) != 0]

"""# Create a graph"""

# Create a new graph
B = nx.Graph()
# We need to specify the nodes for level 0 - this will be our users
B.add_nodes_from(list(set(posts_df.id_owner)), bipartite= 0)
# Then we need to add hashtags nodes as level 1 nodes
B.add_nodes_from(list(set(itertools.chain(*posts_df.tags))), bipartite= 1)

# This quick loop will generate edges between users and hashtags
# Every time someone mentions a #hashtag, a link is created

bi_edges = []
for i in posts_df[['id_owner','tags']].iterrows(): # we do this row-by-row since each row is a post
  id_owner = i[1]['id_owner']
  for j in i[1]['tags']:
    bi_edges.append((id_owner, j)) # edges are appended to a list as a tuple (id_owner, hashtag)

# Let's add the edges to our graph
B.add_edges_from(bi_edges)

# Extract a set of nodes with level 0
top_nodes = {n for n, d in B.nodes(data=True) if d['bipartite']==0}

# the remaining nodes are then level 1
bottom_nodes = set(B) - top_nodes

"""# Now your turn!"""

# Let's project this graph using a weighted projection
G_proj = bipartite.weighted_projected_graph(B, top_nodes)

"""Also, download it in the `Gephi` format. In case you dont hav it installed, get it here: 
https://gephi.org/users/download/

Play a bit around with visualizations. You can also do some of th common analysis (density, community detection etc...)
"""

nx.write_gexf(G_proj, 'G_proj.gexf')

# Again, we can identify communities
bi_communities = community.best_partition(G_proj, resolution = 1)
nx.set_node_attributes(G_proj, bi_communities, 'community')

bi_eigenvector = nx.eigenvector_centrality(G_proj)
nx.set_node_attributes(G_proj, bi_eigenvector, 'eigenvector_centrality')

# Create a new attribute "activity" - or propensity to spam
nx.set_node_attributes(G_proj, dict(posts_df.id_owner.value_counts()), 'activity' )

# Do spammers connect more in terms of spamming about the same stuff?
print(nx.numeric_assortativity_coefficient(G_proj,'activity'))

graph_proj_df = pd.DataFrame(dict(G_proj.nodes(data=True))).T

graph_proj_df.head()

#Find the 5 most central for each identified community
user_per_com = graph_proj_df.groupby('community')['eigenvector_centrality'].nlargest(5)

user_per_com

profile = instaloader.Profile.from_id(L.context, 770263936)

print(profile.biography)
print(profile.username)

nx.write_gexf(G_proj, 'G_proj.gexf')

"""Now lets do some blogmodelling :)"""

# Convert to matrix (numpy.matrix)
mat=nx.adjacency_matrix(G_proj).todense()

# Extract the node labels into a list - useful later on
nodelabels=list(G_proj.nodes())

# Display the sociomatrix
bm.displaySociomatrix(mat,nodelabels)

#
# Do more stuff
#
# Display the network (you might get a warning message the first time - then do it again)
nx.draw(G_proj,with_labels=True,node_color='#FFA0A0', node_size=500)
plt.show()

# Calculate indirect structural equivalence (Hamming distances)
dist=bm.indirectSEhamming(mat) # Also works with bm.indirectSE(mat,method='hamming')

# or calculate correlation-based indirect structural equivalence instead
#corr=bm.indirectSEcorr(mat) # Also works with bm.indirectSE(mat,method='corr')
# As the clustering functionality here works for distances, we need to then convert correlations to distances
# I have added a function for this: corr2dist(mat) - so if you do the following:
#dist=bm.corr2dist(corr)
# ...you should have something that will work as we continue

# Display the distance matrix if you want
print(dist)

# In order to do hierarchical clustering on these distance values, this must first be converted to a condensed distance matrix
# For this, we use scipy.spatial.distance.squareform:

dist_cond = sc.spatial.distance.squareform(dist)

# When doing (agglomerative/bottom-up) hierarchical clustering, we can use one out of many different methods for clustering
# 'single-link' is typically NOT recommended; 'complete', 'average' and 'ward' are more common in this context

# Using the (non-weighted) average clustering approach, we create our clustering object
Z=sch.linkage(dist_cond, method='complete')

# Then we can plot the dendrogram for this particular hierarchical clustering
plt.figure(figsize=(10, 7))
sch.dendrogram(Z,labels=nodelabels) # ...using the nodelabels we extracted in the beginning
plt.show()

# The dendrogram should assist us in choosing a suitable cutoff in the dendrogram
# This cutoff will thus specify the number of positions our blockmodel will have,
# and which nodes/actors that will be part of different partitions


# We use the fcluster function to "cut" the dendrogram at a suitable level. First set that threshold/cutoff value:
threshold=500

partition = sch.fcluster(Z,threshold,'distance')

# This 'partition' object is a 1-dimensional array (numpy.ndarray), indicating (for each node) which position it belongs to

# Have a look at it:
print(partition)

# Check that you got the number of partitions that you wanted
len(np.unique(partition))

# An alternative (and better) way to store this partition is in the form of a dictionary, of lists
# In the blockmodeling library, I have created a function that generates such a dictionary, from your partition list

blockdict=bm.createBlockdict(partition)

# Display it and you will see
bm.displayBlockdict(blockdict,nodelabels)

# Finally, given your original sociomatrix (max),
# the partition you have found (blockdict), and
# the nodelabels, you can display the final blockmodel

bm.displayBlockmodel(mat,blockdict,nodelabels)

# To assist in interpreting this, I have created a function for
# calculating block densities. It returns a numpy ndarray, containing the
# block densities for the partition specified in blockdict
# This blockimage is sorted according to the indices of blockdict,
# i.e. following the same order as the blockmodel above

densityBI=bm.calcDensityBlockimage(mat,blockdict)

print(np.around(densityBI,decimals=2))